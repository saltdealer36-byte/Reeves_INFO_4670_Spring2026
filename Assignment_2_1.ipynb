{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saltdealer36-byte/Reeves_INFO_4670_Spring2026/blob/main/Assignment_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFKagHf5X0xT"
      },
      "source": [
        "#1. Data Quality Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cqU_aeRX4mi"
      },
      "source": [
        "1.1 Using Python (pandas, matplotlib, or seaborn), load and inspect the Assignment 2 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lAnE6ozGXM90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c7a93600-a02b-47ba-cd48-5560cd7c7017"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ba7bb76f-1c25-4beb-aaa8-d348afa9970e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ba7bb76f-1c25-4beb-aaa8-d348afa9970e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Assignment 2 dataset.csv to Assignment 2 dataset.csv\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4CpuCwNuILz"
      },
      "source": [
        "Write code to explore the data distribution (e.g., region, type, year) and check whether there is any bias. Provide both the code and your interpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k2mcSxluIL0",
        "outputId": "347ea477-c3ad-45e5-a9ce-d2c85343843b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " DATA DISTRIBUTION AND BIAS CHECK \n",
            "\n",
            "Column: REGION\n",
            "                     Count  Percentage (%)\n",
            "region                                    \n",
            "WestTexNewMexico       340        1.862605\n",
            "Albany                 338        1.851649\n",
            "BaltimoreWashington    338        1.851649\n",
            "Boise                  338        1.851649\n",
            "Boston                 338        1.851649\n",
            "Atlanta                338        1.851649\n",
            "California             338        1.851649\n",
            "Charlotte              338        1.851649\n",
            "Chicago                338        1.851649\n",
            "CincinnatiDayton       338        1.851649\n",
            "Columbus               338        1.851649\n",
            "DallasFtWorth          338        1.851649\n",
            "Denver                 338        1.851649\n",
            "Detroit                338        1.851649\n",
            "GrandRapids            338        1.851649\n",
            "GreatLakes             338        1.851649\n",
            "HarrisburgScranton     338        1.851649\n",
            "HartfordSpringfield    338        1.851649\n",
            "Houston                338        1.851649\n",
            "Indianapolis           338        1.851649\n",
            "Jacksonville           338        1.851649\n",
            "BuffaloRochester       338        1.851649\n",
            "LasVegas               338        1.851649\n",
            "LosAngeles             338        1.851649\n",
            "MiamiFtLauderdale      338        1.851649\n",
            "Louisville             338        1.851649\n",
            "Nashville              338        1.851649\n",
            "NewOrleansMobile       338        1.851649\n",
            "NewYork                338        1.851649\n",
            "Midsouth               338        1.851649\n",
            "NorthernNewEngland     338        1.851649\n",
            "Orlando                338        1.851649\n",
            "Philadelphia           338        1.851649\n",
            "PhoenixTucson          338        1.851649\n",
            "Pittsburgh             338        1.851649\n",
            "Plains                 338        1.851649\n",
            "Portland               338        1.851649\n",
            "Northeast              338        1.851649\n",
            "RaleighGreensboro      338        1.851649\n",
            "RichmondNorfolk        338        1.851649\n",
            "Sacramento             338        1.851649\n",
            "Roanoke                338        1.851649\n",
            "SanFrancisco           338        1.851649\n",
            "Seattle                338        1.851649\n",
            "SouthCarolina          338        1.851649\n",
            "SanDiego               338        1.851649\n",
            "SouthCentral           338        1.851649\n",
            "Southeast              338        1.851649\n",
            "StLouis                338        1.851649\n",
            "Spokane                338        1.851649\n",
            "Syracuse               338        1.851649\n",
            "Tampa                  338        1.851649\n",
            "TotalUS                338        1.851649\n",
            "West                   338        1.851649\n",
            "------------------------------\n",
            "Column: TYPE\n",
            "              Count  Percentage (%)\n",
            "type                               \n",
            "organic        9127       50.002739\n",
            "conventional   9126       49.997261\n",
            "------------------------------\n",
            "Column: YEAR\n",
            "      Count  Percentage (%)\n",
            "year                       \n",
            "2017   5722       31.346554\n",
            "2016   5616       30.765860\n",
            "2015   5615       30.760381\n",
            "2018   1300        7.121727\n",
            "1904      1        0.005478\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('Assignment 2 dataset.csv')\n",
        "\n",
        "cols_to_check = ['region', 'type', 'year']\n",
        "\n",
        "print(\" DATA DISTRIBUTION AND BIAS CHECK \\n\")\n",
        "\n",
        "for col in cols_to_check:\n",
        "    if col in df.columns:\n",
        "        print(f\"Column: {col.upper()}\")\n",
        "        # Calculate raw counts and percentages\n",
        "        counts = df[col].value_counts()\n",
        "        percentages = df[col].value_counts(normalize=True) * 100\n",
        "\n",
        "        # Combine into a readable summary\n",
        "        summary = pd.DataFrame({'Count': counts, 'Percentage (%)': percentages})\n",
        "        print(summary)\n",
        "        print(\"-\" * 30)\n",
        "    else:\n",
        "        print(f\"Column '{col}' not found in dataset. Check spelling!\")\n",
        "\n",
        "        # This identifies repeated data and gaps. High duplicate counts can lead to overfitting, where the model just memorizes specific rows rather than learning general patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lAUTHNsX7VK"
      },
      "source": [
        "1.2 Write Python code to check for duplicate rows and missing values in the dataset. Show the number of duplicates and missing values for each column. Then, explain (in comments or markdown) how you would handle these issues (e.g., drop, impute, or replace)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5g12xgZIYBpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5dc0053-69e6-4234-a79c-ee42b144fe37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DATA QUALITY SUMMARY ---\n",
            "\n",
            "Total Duplicate Rows: 2\n",
            "\n",
            "Missing Values Per Column:\n",
            "Column 1        0\n",
            "Date            0\n",
            "AveragePrice    0\n",
            "Total Volume    1\n",
            "4046            2\n",
            "4225            1\n",
            "4770            1\n",
            "Total Bags      1\n",
            "Small Bags      2\n",
            "Large Bags      2\n",
            "XLarge Bags     1\n",
            "type            1\n",
            "year            0\n",
            "region          0\n",
            "dtype: int64\n",
            "\n",
            "--- DISTRIBUTION CHECK (Top 5 per Category) ---\n",
            "\n",
            "Top Values for REGION:\n",
            "region\n",
            "WestTexNewMexico       1.862605\n",
            "Albany                 1.851649\n",
            "BaltimoreWashington    1.851649\n",
            "Boise                  1.851649\n",
            "Boston                 1.851649\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Top Values for TYPE:\n",
            "type\n",
            "organic         50.002739\n",
            "conventional    49.997261\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Top Values for YEAR:\n",
            "year\n",
            "2017    31.346554\n",
            "2016    30.765860\n",
            "2015    30.760381\n",
            "2018     7.121727\n",
            "1904     0.005478\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into the variable 'df'\n",
        "df = pd.read_csv('Assignment 2 dataset.csv')\n",
        "\n",
        "print(\"--- DATA QUALITY SUMMARY ---\\n\")\n",
        "\n",
        "# 1. Check for Duplicate Rows\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Total Duplicate Rows: {duplicate_count}\")\n",
        "\n",
        "# 2. Check for Missing Values\n",
        "print(\"\\nMissing Values Per Column:\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "# 3. Quick Distribution for Bias Check (as requested previously)\n",
        "print(\"\\n--- DISTRIBUTION CHECK (Top 5 per Category) ---\")\n",
        "for col in ['region', 'type', 'year']:\n",
        "    if col in df.columns:\n",
        "        print(f\"\\nTop Values for {col.upper()}:\")\n",
        "        print(df[col].value_counts(normalize=True).head(5) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HR99RQCYCAK"
      },
      "source": [
        "1.3 Use Python code to print the number of rows and columns in the dataset (e.g., with df.shape). Based on the dataset size, explain (briefly) whether you think the dataset is sufficient for training a machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qCX1CYR-YEfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef0d5d4-89d5-44cf-e7d9-c4ab55f219cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset contains 18254 rows and 14 columns.\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# Print the shape (rows, columns)\n",
        "rows, cols = df.shape\n",
        "print(f\"The dataset contains {rows} rows and {cols} columns.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XPy7b93Zz7y"
      },
      "source": [
        "#2. Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcDyYNbGZ3s5"
      },
      "source": [
        "2.1 Remove the first column or “Column 1” from the dataset. Treat the ‘year’ variable as nominal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CNhoAKHfZ6iJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7254c299-65ff-42f9-fff6-5377ce409ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First column removed and 'year' converted to string.\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18254 entries, 0 to 18253\n",
            "Data columns (total 13 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Date          18254 non-null  object \n",
            " 1   AveragePrice  18254 non-null  float64\n",
            " 2   Total Volume  18253 non-null  float64\n",
            " 3   4046          18252 non-null  float64\n",
            " 4   4225          18253 non-null  float64\n",
            " 5   4770          18253 non-null  float64\n",
            " 6   Total Bags    18253 non-null  float64\n",
            " 7   Small Bags    18252 non-null  float64\n",
            " 8   Large Bags    18252 non-null  float64\n",
            " 9   XLarge Bags   18253 non-null  float64\n",
            " 10  type          18253 non-null  object \n",
            " 11  year          18254 non-null  object \n",
            " 12  region        18254 non-null  object \n",
            "dtypes: float64(9), object(4)\n",
            "memory usage: 1.8+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# 1. Remove the first column\n",
        "df = df.drop(df.columns[0], axis=1)\n",
        "\n",
        "# 2. Convert year to string\n",
        "if 'year' in df.columns:\n",
        "    df['year'] = df['year'].astype(str)\n",
        "\n",
        "print(\"First column removed and 'year' converted to string.\")\n",
        "print(df.info()) # Verify changes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0CnGYUdZ60p"
      },
      "source": [
        "2.2 Check for duplicate values and remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6jZ92vdvZ82J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e50e896-16fe-4e70-c26a-aa210c79c4e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remaining rows after removing duplicates: 18252\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# Remove duplicates and verify\n",
        "df = df.drop_duplicates()\n",
        "print(f\"Remaining rows after removing duplicates: {len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIVjpXq3Z9Mh"
      },
      "source": [
        "2.3 Check for missing values. If a data record (row) only has a few missing values, replace the missing values with the median of the column feature in that specific “Region” variable. If most column values in a data record are missing, remove the data record."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DjCqQrfKaAEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f003ec3-0ee8-4012-87a7-e8a53076277a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values handled using Regional Median imputation.\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# 1. Remove rows where most values are missing\n",
        "limit = len(df.columns) / 2\n",
        "df = df.dropna(thresh=limit)\n",
        "\n",
        "# 2. Replace remaining missing values with the median of their specific Region\n",
        "num_cols = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "for col in num_cols:\n",
        "    df[col] = df.groupby('region')[col].transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "print(\"Missing values handled using Regional Median imputation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfu21OWmaAf7"
      },
      "source": [
        "2.4 Find the correlation between the variables and describe how the correlated values among the variables impact the model accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QLsEOBvcaDxh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c348ab-5f95-4d1d-bb52-0e1d5578be43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- CORRELATION MATRIX ---\n",
            "              AveragePrice  Total Volume      4046      4225      4770  \\\n",
            "AveragePrice      1.000000     -0.192767 -0.208325 -0.172944 -0.179458   \n",
            "Total Volume     -0.192767      1.000000  0.977863  0.974181  0.872203   \n",
            "4046             -0.208325      0.977863  1.000000  0.926110  0.833390   \n",
            "4225             -0.172944      0.974181  0.926110  1.000000  0.887856   \n",
            "4770             -0.179458      0.872203  0.833390  0.887856  1.000000   \n",
            "Total Bags       -0.177103      0.963047  0.920057  0.905788  0.792315   \n",
            "Small Bags       -0.174742      0.967238  0.925280  0.916032  0.802734   \n",
            "Large Bags       -0.172953      0.880640  0.838646  0.810016  0.698473   \n",
            "XLarge Bags      -0.117604      0.747158  0.699378  0.688810  0.679862   \n",
            "\n",
            "              Total Bags  Small Bags  Large Bags  XLarge Bags  \n",
            "AveragePrice   -0.177103   -0.174742   -0.172953    -0.117604  \n",
            "Total Volume    0.963047    0.967238    0.880640     0.747158  \n",
            "4046            0.920057    0.925280    0.838646     0.699378  \n",
            "4225            0.905788    0.916032    0.810016     0.688810  \n",
            "4770            0.792315    0.802734    0.698473     0.679862  \n",
            "Total Bags      1.000000    0.994334    0.943009     0.804233  \n",
            "Small Bags      0.994334    1.000000    0.902589     0.806845  \n",
            "Large Bags      0.943009    0.902589    1.000000     0.710860  \n",
            "XLarge Bags     0.804233    0.806845    0.710860     1.000000  \n",
            "\n",
            " HIGHLY CORRELATED PAIRS (Above 0.7 or Below -0.7) \n",
            "Small Bags    Total Bags      0.994334\n",
            "Total Bags    Small Bags      0.994334\n",
            "4046          Total Volume    0.977863\n",
            "Total Volume  4046            0.977863\n",
            "              4225            0.974181\n",
            "4225          Total Volume    0.974181\n",
            "Small Bags    Total Volume    0.967238\n",
            "Total Volume  Small Bags      0.967238\n",
            "Total Bags    Total Volume    0.963047\n",
            "Total Volume  Total Bags      0.963047\n",
            "Large Bags    Total Bags      0.943009\n",
            "Total Bags    Large Bags      0.943009\n",
            "4225          4046            0.926110\n",
            "4046          4225            0.926110\n",
            "Small Bags    4046            0.925280\n",
            "4046          Small Bags      0.925280\n",
            "              Total Bags      0.920057\n",
            "Total Bags    4046            0.920057\n",
            "4225          Small Bags      0.916032\n",
            "Small Bags    4225            0.916032\n",
            "Total Bags    4225            0.905788\n",
            "4225          Total Bags      0.905788\n",
            "Large Bags    Small Bags      0.902589\n",
            "Small Bags    Large Bags      0.902589\n",
            "4225          4770            0.887856\n",
            "4770          4225            0.887856\n",
            "Total Volume  Large Bags      0.880640\n",
            "Large Bags    Total Volume    0.880640\n",
            "4770          Total Volume    0.872203\n",
            "Total Volume  4770            0.872203\n",
            "Large Bags    4046            0.838646\n",
            "4046          Large Bags      0.838646\n",
            "4770          4046            0.833390\n",
            "4046          4770            0.833390\n",
            "Large Bags    4225            0.810016\n",
            "4225          Large Bags      0.810016\n",
            "Small Bags    XLarge Bags     0.806845\n",
            "XLarge Bags   Small Bags      0.806845\n",
            "              Total Bags      0.804233\n",
            "Total Bags    XLarge Bags     0.804233\n",
            "4770          Small Bags      0.802734\n",
            "Small Bags    4770            0.802734\n",
            "Total Bags    4770            0.792315\n",
            "4770          Total Bags      0.792315\n",
            "XLarge Bags   Total Volume    0.747158\n",
            "Total Volume  XLarge Bags     0.747158\n",
            "Large Bags    XLarge Bags     0.710860\n",
            "XLarge Bags   Large Bags      0.710860\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# Calculate the correlation\n",
        "correlation_matrix = df.select_dtypes(include=['number']).corr()\n",
        "\n",
        "print(\"--- CORRELATION MATRIX ---\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Identification of highly correlated pairs\n",
        "print(\"\\n HIGHLY CORRELATED PAIRS (Above 0.7 or Below -0.7) \")\n",
        "high_corr = correlation_matrix.unstack().sort_values(ascending=False)\n",
        "high_corr = high_corr[(abs(high_corr) > 0.7) & (high_corr != 1.0)]\n",
        "print(high_corr if not high_corr.empty else \"No highly correlated pairs found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCUOcJyzaEbJ"
      },
      "source": [
        "#3. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn0ad9wIaJ5p"
      },
      "source": [
        "3.1 Describe the variables\n",
        "- Describe all variables in the dataset.\n",
        "- For continuous variables: report **range (min, max), mean, median, and distribution**.\n",
        "- For categorical variables: list unique values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "q-L_L_FweWXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f90c6b2-9e9a-4e80-968a-d92e4b4682f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CONTINUOUS VARIABLES \n",
            "\n",
            "Variable: AveragePrice\n",
            "Range: 0.44 to 3.25\n",
            "Mean: 1.41 | Median: 1.37\n",
            "Skew: 0.58\n",
            "\n",
            "Variable: Total Volume\n",
            "Range: 84.56 to 62505646.52\n",
            "Mean: 850552.31 | Median: 107354.25\n",
            "Skew: 9.01\n",
            "\n",
            "Variable: 4046\n",
            "Range: 0.0 to 22743616.17\n",
            "Mean: 292983.95 | Median: 8645.30\n",
            "Skew: 8.65\n",
            "\n",
            "Variable: 4225\n",
            "Range: 0.0 to 20470572.61\n",
            "Mean: 295122.55 | Median: 29056.73\n",
            "Skew: 8.94\n",
            "\n",
            "Variable: 4770\n",
            "Range: 0.0 to 2546439.11\n",
            "Mean: 22837.27 | Median: 184.99\n",
            "Skew: 10.16\n",
            "\n",
            "Variable: Total Bags\n",
            "Range: 0.0 to 19373134.37\n",
            "Mean: 239613.96 | Median: 39738.53\n",
            "Skew: 9.76\n",
            "\n",
            "Variable: Small Bags\n",
            "Range: 0.0 to 13384586.8\n",
            "Mean: 182178.42 | Median: 26362.82\n",
            "Skew: 9.54\n",
            "\n",
            "Variable: Large Bags\n",
            "Range: 0.0 to 5719096.61\n",
            "Mean: 54332.33 | Median: 2647.71\n",
            "Skew: 9.80\n",
            "\n",
            "Variable: XLarge Bags\n",
            "Range: 0.0 to 551693.65\n",
            "Mean: 3106.09 | Median: 0.00\n",
            "Skew: 13.14\n",
            "\n",
            " CATEGORICAL VARIABLES \n",
            "\n",
            "Variable: Date\n",
            "Unique Values: ['12-27-2015' '12-20-2015' '12-13-2015' '12-6-2015' '11-29-2015'\n",
            " '11-22-2015' '11-15-2015' '11-8-2015' '11-1-2015' '10-25-2015'\n",
            " '10-18-2015' '10-11-2015' '10-4-2015' '9-27-2015' '9-20-2015' '9-13-2015'\n",
            " '9-6-2015' '8-30-2015' '8-23-2015' '8-16-2015' '8-9-2015' '8-2-2015'\n",
            " '7-26-2015' '7-19-2015' '7-12-2015' '7-5-2015' '6-28-2015' '6-21-2015'\n",
            " '6-14-2015' '6-7-2015' '5-31-2015' '5-24-2015' '5-17-2015' '5-10-2015'\n",
            " '5-3-2015' '4-26-2015' '4-19-2015' '4-12-2015' '4-5-2015' '3-29-2015'\n",
            " '3-22-2015' '3-15-2015' '3-8-2015' '3-1-2015' '2-22-2015' '2-15-2015'\n",
            " '2-8-2015' '2-1-2015' '1-25-2015' '1-18-2015' '1-11-2015' '1-4-2015'\n",
            " '12-25-2016' '12-18-2016' '12-11-2016' '12-4-2016' '11-27-2016'\n",
            " '11-20-2016' '11-13-2016' '11-6-2016' '10-30-2016' '10-23-2016'\n",
            " '10-16-2016' '10-9-2016' '10-2-2016' '9-25-2016' '9-18-2016' '9-11-2016'\n",
            " '9-4-2016' '8-28-2016' '8-21-2016' '8-14-2016' '8-7-2016' '7-31-2016'\n",
            " '7-24-2016' '7-17-2016' '7-10-2016' '7-3-2016' '6-26-2016' '6-19-2016'\n",
            " '6-12-2016' '6-5-2016' '5-29-2016' '5-22-2016' '5-15-2016' '5-8-2016'\n",
            " '5-1-2016' '4-24-2016' '4-17-2016' '4-10-2016' '4-3-2016' '3-27-2016'\n",
            " '3-20-2016' '3-13-2016' '3-6-2016' '2-28-2016' '2-21-2016' '2-14-2016'\n",
            " '2-7-2016' '1-31-2016' '1-24-2016' '1-17-2016' '1-10-2016' '1-3-2016'\n",
            " '12-31-2017' '12-24-2017' '12-17-2017' '12-10-2017' '12-3-2017'\n",
            " '11-26-2017' '11-19-2017' '11-12-2017' '11-5-2017' '10-29-2017'\n",
            " '10-22-2017' '10-15-2017' '10-8-2017' '10-1-2017' '9-24-2017' '9-17-2017'\n",
            " '9-10-2017' '9-3-2017' '8-27-2017' '8-20-2017' '8-13-2017' '8-6-2017'\n",
            " '7-30-2017' '7-23-2017' '7-16-2017' '7-9-2017' '7-2-2017' '6-25-2017'\n",
            " '6-18-2017' '6-11-2017' '6-4-2017' '5-28-2017' '5-21-2017' '5-14-2017'\n",
            " '5-7-2017' '4-30-2017' '4-23-2017' '4-16-2017' '4-9-2017' '4-2-2017'\n",
            " '3-26-2017' '3-19-2017' '3-12-2017' '3-5-2017' '2-26-2017' '2-19-2017'\n",
            " '2-12-2017' '2-5-2017' '1-29-2017' '1-22-2017' '1-15-2017' '1-8-2017'\n",
            " '1-1-2017' '3-25-2018' '3-18-2018' '3-11-2018' '3-4-2018' '2-25-2018'\n",
            " '2-18-2018' '2-11-2018' '2-4-2018' '1-28-2018' '1-21-2018' '1-14-2018'\n",
            " '1-7-2018' '1-21-1904']\n",
            "\n",
            "Variable: type\n",
            "Unique Values: ['conventional' 'organic']\n",
            "\n",
            "Variable: year\n",
            "Unique Values: ['2015' '2016' '2017' '2018' '1904']\n",
            "\n",
            "Variable: region\n",
            "Unique Values: ['Albany' 'Atlanta' 'BaltimoreWashington' 'Boise' 'Boston'\n",
            " 'BuffaloRochester' 'California' 'Charlotte' 'Chicago' 'CincinnatiDayton'\n",
            " 'Columbus' 'DallasFtWorth' 'Denver' 'Detroit' 'GrandRapids' 'GreatLakes'\n",
            " 'HarrisburgScranton' 'HartfordSpringfield' 'Houston' 'Indianapolis'\n",
            " 'Jacksonville' 'LasVegas' 'LosAngeles' 'Louisville' 'MiamiFtLauderdale'\n",
            " 'Midsouth' 'Nashville' 'NewOrleansMobile' 'NewYork' 'Northeast'\n",
            " 'NorthernNewEngland' 'Orlando' 'Philadelphia' 'PhoenixTucson'\n",
            " 'Pittsburgh' 'Plains' 'Portland' 'RaleighGreensboro' 'RichmondNorfolk'\n",
            " 'Roanoke' 'Sacramento' 'SanDiego' 'SanFrancisco' 'Seattle'\n",
            " 'SouthCarolina' 'SouthCentral' 'Southeast' 'Spokane' 'StLouis' 'Syracuse'\n",
            " 'Tampa' 'TotalUS' 'West' 'WestTexNewMexico']\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# 1. Continuous variables (Numerical)\n",
        "print(\" CONTINUOUS VARIABLES \")\n",
        "cont_cols = df.select_dtypes(include=['number']).columns\n",
        "for col in cont_cols:\n",
        "    print(f\"\\nVariable: {col}\")\n",
        "    print(f\"Range: {df[col].min()} to {df[col].max()}\")\n",
        "    print(f\"Mean: {df[col].mean():.2f} | Median: {df[col].median():.2f}\")\n",
        "    # Quick text-based distribution summary\n",
        "    print(f\"Skew: {df[col].skew():.2f}\")\n",
        "\n",
        "# 2. Categorical variables\n",
        "print(\"\\n CATEGORICAL VARIABLES \")\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "for col in cat_cols:\n",
        "    print(f\"\\nVariable: {col}\")\n",
        "    print(f\"Unique Values: {df[col].unique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAYz1-vDeYz-"
      },
      "source": [
        "3.2 Inspect the earliest recorded date\n",
        "- Find the earliest `Date`.\n",
        "- Check if there are avocado prices recorded from the earliest date up to 2010.\n",
        "- Comment: does the earliest data point look reasonable? Keep or remove?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tjdWkRxweYFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1875d7c-657e-4088-a6c9-a85582da2f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Earliest recorded date: 1904-01-21 00:00:00\n",
            "Number of records from 2010 or earlier: 1\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# Convert Date to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Find the earliest date\n",
        "earliest_date = df['Date'].min()\n",
        "print(f\"Earliest recorded date: {earliest_date}\")\n",
        "\n",
        "# Check for records up to 2010\n",
        "pre_2010 = df[df['Date'].dt.year <= 2010]\n",
        "print(f\"Number of records from 2010 or earlier: {len(pre_2010)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYYXKOJDf-bt"
      },
      "source": [
        "3.3 Highest average price\n",
        "- Find the highest value in \"AveragePrice\".\n",
        "- Report which region it belongs to.\n",
        "- Describe how you obtained the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OMfJk4NKf6Vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b0365e-6cf8-432e-d9b9-1cda9e3f3cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highest Average Price: $3.25\n",
            "Region: SanFrancisco\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# 1. Find the highest value in \"AveragePrice\"\n",
        "highest_price = df['AveragePrice'].max()\n",
        "\n",
        "\n",
        "highest_price_index = df['AveragePrice'].idxmax()\n",
        "region_at_peak = df.loc[highest_price_index, 'region']\n",
        "\n",
        "print(f\"Highest Average Price: ${highest_price}\")\n",
        "print(f\"Region: {region_at_peak}\")\n",
        "\n",
        "# Description of method:\n",
        "# The result was obtained by using the .max() function to identify the peak numerical value.\n",
        "# I then used .idxmax() to find the specific row index for that price and\n",
        "# cross-referenced it with the 'region' column using .loc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwRFtc8YgCBO"
      },
      "source": [
        "3.4 Highest total volume\n",
        "- Find the highest total volume of avocados.\n",
        "- Report which region it belongs to.\n",
        "- Describe how you obtained the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tYzZo6C4gQym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c36becc-e043-49db-ff69-080432206f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highest Total Volume: 62,505,646.52\n",
            "Found in Region: TotalUS\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# 1. Find the highest value in \"Total Volume\"\n",
        "max_volume = df['Total Volume'].max()\n",
        "\n",
        "# 2. Report the region\n",
        "max_vol_index = df['Total Volume'].idxmax()\n",
        "region_at_max_vol = df.loc[max_vol_index, 'region']\n",
        "\n",
        "print(f\"Highest Total Volume: {max_volume:,.2f}\")\n",
        "print(f\"Found in Region: {region_at_max_vol}\")\n",
        "\n",
        "# 3. Method Description:\n",
        "# Similar to the price analysis, I used .max() for the value and .idxmax()\n",
        "# combined with .loc to pinpoint the geographical region responsible for\n",
        "# this volume peak.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}